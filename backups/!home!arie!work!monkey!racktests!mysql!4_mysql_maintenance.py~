from strato.tests import monkeytest
from strato.donkey.guests import guestcirros
from strato.common.multithreading import waitforpredicate
from strato.whiteboxtest.infra.suite import TS_ASSERT
from strato.whiteboxtest.infra.suite import TS_ASSERT_PREDICATE_TIMEOUT, TS_ASSERT_EQUALS
from strato_kv.clustermanagement import consts as strato_kv_consts
import random
import logging
import time


class Test(monkeytest.MonkeyTest):

    EXEC_META = {
        'PRIORITY': 1,
        'COMPLEXITY': 2,
        'KEYWORDS': ['cluster_manager_subsystem', 'system_test'],
        'TEST_TYPE': 'functionality'
    }

    ABORT_TEST_TIMEOUT = 60 * 60

    NUM_OF_HOSTS = 5

    HOSTS = {'cluster1': {
        'node%i' % i: dict(rootfs='rootfs-star')
        for i in range(NUM_OF_HOSTS)
    }}

    NETWORKS = {'sourceCluster':
                {'homeNetwork': dict(subnet='auto')}}

    NUM_OF_VMS = 2

    PREDICATE_TIMEOUT = 200
    PREDICATE_TIMEOUT_INTERVAL = 5

    def run(self):
        self.cluster1.vmcontrol.createVMs(numOfVMs=self.NUM_OF_VMS, guestType=guestcirros.GuestCirros)
        logging.progress('Waiting for 3 mysql healthy nodes')
        TS_ASSERT_PREDICATE_TIMEOUT(self._isMysqlClusterHealthy,
                                    TS_timeout=Test.PREDICATE_TIMEOUT,
                                    TS_interval=Test.PREDICATE_TIMEOUT_INTERVAL)
        logging.progress('1. Crashing a node with mysql, and forcing it into maitnenance')
        self.cluster1.mancalacontrol.setTableVersions()
        mysqlNodes = self._getHealthyMysqlNodes()
        hostnameToCrash = random.choice(mysqlNodes)
        logging.info('Going to crash %s', hostnameToCrash)
        nodeToCrash = self.cluster1.getHostByName(hostnameToCrash.replace('strato', ''))
        self.cluster1.nodecontrol.asyncKillNodeAndKeepItDown(nodeToCrash)
        TS_ASSERT_PREDICATE_TIMEOUT(self._isMysqlClusterHealthy,
                                    expectedHealthyMysqlInstances=2,
                                    TS_timeout=Test.PREDICATE_TIMEOUT,
                                    TS_interval=Test.PREDICATE_TIMEOUT_INTERVAL)
        time.sleep(60)
        TS_ASSERT_EQUALS(self._getMysqlStatusOnNodes()[hostnameToCrash], 'failed')
        self.cluster1.stratoapi.moveNodeToMaintenance(nodeToCrash.nodeproperties.getUuid(), force=True)
        TS_ASSERT_PREDICATE_TIMEOUT(self._isMysqlClusterHealthy,
                                    TS_timeout=Test.PREDICATE_TIMEOUT,
                                    TS_interval=Test.PREDICATE_TIMEOUT_INTERVAL)
        self.cluster1.mancalacontrol.waitForPoolToBeHealthy()

        logging.progress('2. Sabotaging mysql on a node, and taking it gracefully into maitnenance')
        self.cluster1.mancalacontrol.setTableVersions()
        mysqlNodes = self._getHealthyMysqlNodes()
        hostnameToSabotage = random.choice(mysqlNodes)
        logging.info('Going to sabotage mysql on %s', hostnameToSabotage)
        nodeToSabotage = self.cluster1.getHostByName(hostnameToSabotage.replace('strato', ''))
        self.sabotageMysqlOnNode(nodeToSabotage)
        TS_ASSERT_PREDICATE_TIMEOUT(self._isMysqlClusterHealthy,
                                    expectedHealthyMysqlInstances=2,
                                    TS_timeout=Test.PREDICATE_TIMEOUT,
                                    TS_interval=Test.PREDICATE_TIMEOUT_INTERVAL)
        self.cluster1.stratoapi.moveNodeToMaintenance(nodeToSabotage.nodeproperties.getUuid())
        TS_ASSERT_PREDICATE_TIMEOUT(self._isMysqlClusterHealthy,
                                    TS_timeout=Test.PREDICATE_TIMEOUT,
                                    TS_interval=Test.PREDICATE_TIMEOUT_INTERVAL)
        self.stopMysqlSabotageOnNode(nodeToSabotage)
        self.cluster1.mancalacontrol.waitForPoolToBeHealthy()

        logging.progress('3. Sabotaging mysql on 2 nodes, and trying to take the third gracefully into maitnenance')
        import pdb
        pdb.set_trace()
        mysqlNodes = self._getHealthyMysqlNodes()
        hostnameToKeep = random.choice(mysqlNodes)
        hostnamesToSabotage = [hostname for hostname in mysqlNodes if hostname != hostnameToKeep]
        logging.info('Going to sabotage mysql on %s', hostnamesToSabotage)
        for hostnameToSabotage in hostnamesToSabotage:
            nodeToSabotage = self.cluster1.getHostByName(hostnameToSabotage.replace('strato', ''))
            self.sabotageMysqlOnNode(nodeToSabotage)
        try:
            nodeToKeep = self.cluster1.getHostByName(hostnameToKeep.replace('strato', ''))
            self.cluster1.stratoapi.moveNodeToMaintenance(nodeToKeep.nodeproperties.getUuid())
        except Exception as e:
            import pdb
            pdb.set_trace()
        for hostnameToSabotage in hostnamesToSabotage:
            nodeToSabotage = self.cluster1.getHostByName(hostnameToSabotage.replace('strato', ''))
            self.stopMysqlSabotageOnNode(nodeToSabotage)
        TS_ASSERT_PREDICATE_TIMEOUT(self._isMysqlClusterHealthy,
                                    TS_timeout=Test.PREDICATE_TIMEOUT,
                                    TS_interval=Test.PREDICATE_TIMEOUT_INTERVAL)
        self.cluster1.vmcontrol.startAllVMs()

        logging.progress('4. Taking down nodes on by one, last one should fail')
        mysqlNodes = self._getHealthyMysqlNodes()
        logging.info('Going to take down all nodes (%s)', mysqlNodes)
        for hostnameToTakeDown in mysqlNodes[:-1]:
            nodeToTakeDown = self.cluster1.getHostByName(hostnameToTakeDown.replace('strato', ''))
            self.cluster1.stratoapi.moveNodeToMaintenance(nodeToTakeDown.nodeproperties.getUuid())
        TS_ASSERT_PREDICATE_TIMEOUT(self._isMysqlClusterHealthy,
                                    expectedHealthyMysqlInstances=1,
                                    TS_timeout=Test.PREDICATE_TIMEOUT,
                                    TS_interval=Test.PREDICATE_TIMEOUT_INTERVAL)
        hostnameToTakeDown = self.cluster1.getHostByName(hostnameToTakeDown.replace('strato', ''))[-1]
        nodeToTakeDown = self.cluster1.getHostByName(hostnameToTakeDown.replace('strato', ''))
        try:
            self.cluster1.stratoapi.moveNodeToMaintenance(nodeToTakeDown.nodeproperties.getUuid())
        except Exception as e:
            import pdb
            pdb.set_trace()

    def _getMysqlStatusOnNodes(self):
        mysqlStatus = [service for service in self.cluster1.stratoapi.clusterServiceList() if service['name'] == 'mysql']
        TS_ASSERT_EQUALS(1, len(mysqlStatus))
        return mysqlStatus[0]['nodes']

    def _getHealthyMysqlNodes(self):
        return [node for node, health in self._getMysqlStatusOnNodes().iteritems() if health == 'active']

    def _isMysqlClusterHealthy(self, expectedHealthyMysqlInstances=3):
        mysqlNodes = self._getMysqlStatusOnNodes()
        logging.info("Mysql Nodes are: %s", mysqlNodes)
        return len([mysqlNode for mysqlNode, status in mysqlNodes.iteritems() if status == 'active']) == expectedHealthyMysqlInstances

    def sabotageMysqlOnNode(self, node):
        try:
            node.ssh.run.script("mv /etc/init.d/mysql /etc/init.d/mysql-disabled")
            node.ssh.run.script("systemctl restart mysql.service --no-block")
        except:
            logging.warning("Service %s is probably already disabled on node %s" % ('mysql', node.name))

    def stopMysqlSabotageOnNode(self, node):
        try:
            node.ssh.run.script("mv /etc/init.d/mysql-disabled /etc/init.d/mysql")
            node.ssh.run.script("systemctl restart mysql.service --no-block")
        except:
            logging.warning("Service %s is probably not disabled on node %s" % ('mysql', node.name))
